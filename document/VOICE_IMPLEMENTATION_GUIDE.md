# ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™” ì‹œìŠ¤í…œ êµ¬í˜„ ê°€ì´ë“œ

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### ì‚¬ì „ ìš”êµ¬ì‚¬í•­

1. **NVIDIA GPU ë“œë¼ì´ë²„ & CUDA**
   ```bash
   # NVIDIA ë“œë¼ì´ë²„ ë²„ì „ í™•ì¸
   nvidia-smi
   
   # CUDA ë²„ì „ í™•ì¸ (12.x ê¶Œì¥)
   nvcc --version
   ```

2. **Docker & NVIDIA Container Toolkit**
   ```bash
   # Docker ë²„ì „
   docker --version
   
   # GPU í…ŒìŠ¤íŠ¸
   docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
   ```

3. **Ollama ì„¤ì¹˜ ë° ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**
   ```bash
   # qwen2.5 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
   ollama pull qwen2.5:7b-instruct
   
   # ëª¨ë¸ í™•ì¸
   ollama list
   ```

### 1ë‹¨ê³„: í™˜ê²½ ì„¤ì •

```bash
# ì €ì¥ì†Œ í´ë¡  (ì´ë¯¸ ì™„ë£Œ)
cd D:\CursorSpace\DiscordBeeuBot

# .env íŒŒì¼ ì„¤ì •
# DISCORD_TOKEN, WORKER_SHARED_SECRET ë“± ì„¤ì •
```

### 2ë‹¨ê³„: faster-whisper ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìë™)

ì²« ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤:
- `medium` ëª¨ë¸: ~1.5GB
- ì €ì¥ ìœ„ì¹˜: `/app/models` (ì»¨í…Œì´ë„ˆ ë‚´ë¶€)

ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (ì„ íƒì‚¬í•­):
```bash
# Python í™˜ê²½ì—ì„œ
pip install faster-whisper
python -c "from faster_whisper import WhisperModel; WhisperModel('medium')"
```

### 3ë‹¨ê³„: ì„œë¹„ìŠ¤ ì‹œì‘

```bash
# ìŒì„± ì‹œìŠ¤í…œ ì „ì²´ ì‹œì‘
docker-compose -f docker-compose.voice.yml up -d

# ë¡œê·¸ í™•ì¸
docker-compose -f docker-compose.voice.yml logs -f
```

### 4ë‹¨ê³„: í—¬ìŠ¤ì²´í¬

```bash
# ASR ì„œë¹„ìŠ¤
curl http://localhost:5005/health

# Ollama
curl http://localhost:11434/api/tags

# Gateway
curl http://localhost:8001/health
```

## ğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### ASR í…ŒìŠ¤íŠ¸

```bash
# í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ (WAV, 16kHz, mono)
# test_audio.wav

# ë™ê¸° ë³€í™˜ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:5005/transcribe \
  -F "audio_file=@test_audio.wav" \
  -F "language=ko"

# ìŠ¤íŠ¸ë¦¬ë° ë³€í™˜ í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:5005/transcribe-stream \
  -F "audio_file=@test_audio.wav" \
  -F "language=ko"
```

### LLM í…ŒìŠ¤íŠ¸

```bash
# Ollama ì§ì ‘ í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b-instruct",
  "prompt": "ì•ˆë…•í•˜ì„¸ìš”",
  "stream": true
}'
```

### E2E í…ŒìŠ¤íŠ¸

Discordì—ì„œ:
```
1. /join - ìŒì„± ì±„ë„ ì°¸ì—¬
2. /record - ë…¹ìŒ ì‹œì‘
3. (ë§í•˜ê¸°)
4. /stop - ë…¹ìŒ ì¤‘ì§€ ë° ë³€í™˜
```

## ğŸ”§ ìµœì í™” ê°€ì´ë“œ

### ì§€ì—° ê°ì†Œ (ì •í™•ë„ trade-off)

```yaml
# docker-compose.voice.yml ìˆ˜ì •
asr:
  environment:
    - MODEL_NAME=small              # medium â†’ small
    - BEAM_SIZE=1                   # 2 â†’ 1
    - WINDOW_S=0.9                  # 1.0 â†’ 0.9

gateway:
  environment:
    - LLM_NUM_PREDICT=40            # 60 â†’ 40
```

**ì˜ˆìƒ íš¨ê³¼**:
- E2FT: 1.25-2.2s â†’ 0.8-1.5s
- ì •í™•ë„: 90-95% â†’ 85-90%
- VRAM: 6-8GB â†’ 4-5GB

### ì •í™•ë„ í–¥ìƒ (ì§€ì—° trade-off)

```yaml
asr:
  environment:
    - MODEL_NAME=large-v3           # medium â†’ large-v3
    - COMPUTE_TYPE=float16          # int8_float16 â†’ float16
    - BEAM_SIZE=3                   # 2 â†’ 3
```

**ì˜ˆìƒ íš¨ê³¼**:
- E2FT: 1.25-2.2s â†’ 2-3s
- ì •í™•ë„: 90-95% â†’ 95-98%
- VRAM: 6-8GB â†’ 10-12GB (âš ï¸ RTX 2070 16GB í•œê³„ ê·¼ì ‘)

### ë©”ëª¨ë¦¬ ì ˆì•½

```yaml
llm:
  environment:
    - OLLAMA_MAX_LOADED_MODELS=1   # 1ê°œë§Œ ë¡œë“œ
    - OLLAMA_NUM_PARALLEL=1        # ë™ì‹œ ìš”ì²­ 1ê°œ

gateway:
  environment:
    - MAX_CONTEXT_TURNS=4          # 6 â†’ 4 (ì»¨í…ìŠ¤íŠ¸ ì¤„ì„)
```

## ğŸ› ë¬¸ì œ í•´ê²°

### GPU ë¯¸ì¸ì‹

```bash
# NVIDIA Container Toolkit í™•ì¸
docker run --rm --gpus all ubuntu nvidia-smi

# ì‹¤íŒ¨ ì‹œ ì¬ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### ASR ì´ˆê¸°í™” ëŠë¦¼

**ì›ì¸**: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì²« ì‹¤í–‰)

**í•´ê²°**:
```bash
# ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
docker exec -it libra-asr python -c "
from faster_whisper import WhisperModel
WhisperModel('medium', device='cuda', compute_type='int8_float16')
"
```

### ìŒì„± ëŠê¹€/ì¤‘ë³µ

**ì¦ìƒ**: ASR ê²°ê³¼ê°€ ì¤‘ë³µë˜ê±°ë‚˜ ëŠê¹€

**í•´ê²°**:
```yaml
asr:
  environment:
    - WINDOW_OVERLAP_MS=200        # 120 â†’ 200 (ì˜¤ë²„ë© ì¦ê°€)
    - NO_SPEECH_THRESHOLD=0.5      # 0.6 â†’ 0.5 (ì„ê³„ê°’ ë‚®ì¶¤)
```

### LLM ì²« í† í° ëŠë¦¼

**ì›ì¸**: í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¹€, ì»¨í…ìŠ¤íŠ¸ ê³¼ë‹¤

**í•´ê²°**:
```yaml
gateway:
  environment:
    - MAX_CONTEXT_TURNS=4          # 6 â†’ 4
    - LLM_NUM_PREDICT=40           # 60 â†’ 40
```

### VRAM ë¶€ì¡±

**ì¦ìƒ**: CUDA out of memory

**í•´ê²° ë°©ë²•**:

1. **ASR ëª¨ë¸ ë‹¤ìš´ê·¸ë ˆì´ë“œ**
   ```yaml
   MODEL_NAME=small               # medium â†’ small
   ```

2. **Compute Type ë³€ê²½**
   ```yaml
   COMPUTE_TYPE=int8              # int8_float16 â†’ int8
   ```

3. **ì„œë¹„ìŠ¤ ë¶„ë¦¬ ì‹¤í–‰**
   ```bash
   # ASRë§Œ GPU ì‚¬ìš©
   docker-compose -f docker-compose.voice.yml up -d asr postgres redis
   
   # OllamaëŠ” CPU ëª¨ë“œë¡œ
   OLLAMA_DISABLE_GPU=1 ollama serve
   ```

### WSL ë©”ëª¨ë¦¬ ë¶€ì¡± (Windows)

```ini
# C:\Users\<ì‚¬ìš©ì>\.wslconfig
[wsl2]
memory=12GB
swap=8GB
localhostForwarding=true
```

ì¬ì‹œì‘:
```powershell
wsl --shutdown
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### GPU ì‚¬ìš©ë¥ 

```bash
# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë©”ëª¨ë¦¬ ì¶”ì 
nvidia-smi --query-gpu=memory.used,memory.free,memory.total --format=csv --loop=1
```

### ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤

```bash
# CPU/ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
docker stats libra-asr libra-ollama libra-gateway
```

### ë ˆì´í„´ì‹œ ì¸¡ì •

```bash
# ASR ë ˆì´í„´ì‹œ (ì»¨í…Œì´ë„ˆ ë¡œê·¸)
docker logs libra-asr | grep "ë³€í™˜ ì™„ë£Œ"

# E2FT ì¸¡ì • (Gateway ë¡œê·¸)
docker logs libra-gateway | grep "E2FT"
```

## ğŸ”„ ë‹¤ìŒ ë‹¨ê³„

### Phase 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ASR ì„œë¹„ìŠ¤ ë¹Œë“œ ë° ì‹œì‘
- [ ] Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (`qwen2.5:7b-instruct`)
- [ ] GPU ì¸ì‹ í™•ì¸
- [ ] í—¬ìŠ¤ì²´í¬ ì„±ê³µ
- [ ] í…ŒìŠ¤íŠ¸ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜ ì„±ê³µ
- [ ] ë ˆì´í„´ì‹œ ì¸¡ì • (< 2ì´ˆ)

### Phase 2: Gateway êµ¬í˜„

- [ ] Discord Voice ìˆ˜ì‹  íŒŒì´í”„ë¼ì¸
- [ ] VAD ê¸°ë°˜ ì²­í¬ ë¶„í• 
- [ ] ASR ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
- [ ] LLM ì—°ë™
- [ ] TTS í†µí•©
- [ ] Discord Voice ì†¡ì‹ 

### Phase 3: í”„ë¡œë•ì…˜ ì¤€ë¹„

- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ê°•í™”
- [ ] ì¬ì‹œë„ ë¡œì§
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- [ ] ë¡œê¹… ìµœì í™”
- [ ] ë³´ì•ˆ ê°•í™”
- [ ] ë¶€í•˜ í…ŒìŠ¤íŠ¸

## ğŸ“š ì°¸ê³  ìë£Œ

- [faster-whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
- [Silero VAD](https://github.com/snakers4/silero-vad)
- [Ollama Documentation](https://ollama.ai/docs)
- [Discord.js Voice](https://discordjs.guide/voice/)

## ğŸ¯ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (RTX 2070)

### ì‹¤ì¸¡ ë°ì´í„° (ì˜ˆìƒ)

| êµ¬ì„± | ASR | TTFT | E2FT | ì •í™•ë„ | VRAM |
|------|-----|------|------|--------|------|
| small+int8 | 0.5-0.9s | 0.3-0.6s | 0.8-1.5s | 85% | 4GB |
| **medium+int8_float16** | **0.9-1.4s** | **0.35-0.8s** | **1.25-2.2s** | **90-95%** | **6-8GB** |
| large-v3+float16 | 1.5-2.5s | 0.35-0.8s | 2-3.5s | 95-98% | 10-12GB |

**ê¶Œì¥ ì„¤ì •**: medium + int8_float16 (ë°¸ëŸ°ìŠ¤ ìµœì )

