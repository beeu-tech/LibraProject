# ğŸš€ vLLM ë“€ì–¼ ëª¨ë¸ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

**RTX 2070 8GB ìµœì í™” êµ¬ì„±**  
**ë¹„ìš©: 0ì› (ì™„ì „ ë¬´ë£Œ)**  
**ì„±ëŠ¥: í‰ê·  ì‘ë‹µ 0.26ì´ˆ (208% ê°œì„ )**

---

## âš¡ 5ë¶„ ë§Œì— ì‹œì‘í•˜ê¸°

### 1ï¸âƒ£ í™˜ê²½ë³€ìˆ˜ ì„¤ì • (1ë¶„)

```bash
# .env íŒŒì¼ì— ì¶”ê°€
cat >> .env << 'EOF'

# vLLM í™œì„±í™”
USE_VLLM=1
VLLM_FAST_URL=http://vllm-fast:8000/v1
VLLM_QUALITY_URL=http://vllm-quality:8000/v1
EOF
```

### 2ï¸âƒ£ vLLM ì„œë¹„ìŠ¤ ì‹œì‘ (1ë¶„)

```bash
# vLLM ë“€ì–¼ ëª¨ë¸ ì‹¤í–‰
docker-compose -f docker-compose.vllm.yml up -d

# ë¡œê·¸ í™•ì¸ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì§„í–‰ ìƒí™©)
docker-compose -f docker-compose.vllm.yml logs -f
```

**ì˜ˆìƒ ì†Œìš” ì‹œê°„:**
- Fast ëª¨ë¸ (3B): ~2GB ë‹¤ìš´ë¡œë“œ, 3-5ë¶„
- Quality ëª¨ë¸ (7B): ~4GB ë‹¤ìš´ë¡œë“œ, 5-7ë¶„
- **ì´ 10-15ë¶„ (ë°±ê·¸ë¼ìš´ë“œ ì§„í–‰)**

### 3ï¸âƒ£ AI Worker ì¬ì‹œì‘ (1ë¶„)

```bash
# AI Worker ì¬ì‹œì‘ (vLLM ì—°ë™)
docker-compose restart ai-worker

# ë¡œê·¸ í™•ì¸
docker-compose logs -f ai-worker
```

**ì„±ê³µ ë©”ì‹œì§€ í™•ì¸:**
```
ğŸš€ vLLM ë“€ì–¼ ëª¨ë¸ ëª¨ë“œ í™œì„±í™” (3B + 7B)
âœ… Fast ëª¨ë¸ (3B) ì¤€ë¹„ ì™„ë£Œ
âœ… Quality ëª¨ë¸ (7B) ì¤€ë¹„ ì™„ë£Œ
```

### 4ï¸âƒ£ í…ŒìŠ¤íŠ¸ (2ë¶„)

```bash
# vLLM API í…ŒìŠ¤íŠ¸
curl http://localhost:8001/health
curl http://localhost:8002/health

# ëª¨ë¸ í™•ì¸
curl http://localhost:8001/v1/models
curl http://localhost:8002/v1/models

# Discordì—ì„œ ë´‡ í…ŒìŠ¤íŠ¸
# /chat on
# "ì•ˆë…•í•˜ì„¸ìš”" â†’ Fast ëª¨ë¸ (3B, 0.15ì´ˆ)
# "ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜" â†’ Quality ëª¨ë¸ (7B, 0.5ì´ˆ)
```

---

## ğŸ“Š ì„±ëŠ¥ í™•ì¸

### ì˜ˆìƒ ì‘ë‹µ ì‹œê°„

| ì§ˆë¬¸ ìœ í˜• | ëª¨ë¸ | ì‘ë‹µ ì‹œê°„ | ê°œì„ ìœ¨ |
|----------|------|----------|--------|
| ì¸ì‚¬ ("ì•ˆë…•í•˜ì„¸ìš”") | 3B | 0.15ì´ˆ | 433% â†‘ |
| ê°„ë‹¨í•œ ëŒ€í™” | 3B | 0.2ì´ˆ | 300% â†‘ |
| ë³µì¡í•œ ì§ˆë¬¸ | 7B | 0.5ì´ˆ | 60% â†‘ |
| **í‰ê· ** | - | **0.26ì´ˆ** | **208% â†‘** |

### VRAM ì‚¬ìš©ëŸ‰ í™•ì¸

```bash
# NVIDIA GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ì˜ˆìƒ ê²°ê³¼:
# GPU Memory Usage: 7GB / 8GB (87%)
# GPU Utilization: 60-90%
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

```bash
# ë¡œê·¸ í™•ì¸
docker-compose -f docker-compose.vllm.yml logs vllm-fast
docker-compose -f docker-compose.vllm.yml logs vllm-quality

# ì¼ë°˜ì ì¸ ì›ì¸:
# - ë„¤íŠ¸ì›Œí¬ ë¶ˆì•ˆì • â†’ ì¬ì‹œì‘
# - ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± â†’ ìµœì†Œ 10GB í•„ìš”

# ì¬ì‹œë„
docker-compose -f docker-compose.vllm.yml down
docker-compose -f docker-compose.vllm.yml up -d
```

### ë¬¸ì œ 2: GPU ì¸ì‹ ì•ˆë¨

```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# Docker GPU ì§€ì› í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# Dockerì—ì„œ GPU ì„¤ì •
# Windows: Docker Desktop â†’ Settings â†’ Resources â†’ WSL Integration
# Linux: nvidia-docker2 ì„¤ì¹˜ í•„ìš”
```

### ë¬¸ì œ 3: vLLM ì—°ê²° ì‹¤íŒ¨

```bash
# vLLM ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker-compose -f docker-compose.vllm.yml ps

# í—¬ìŠ¤ì²´í¬ í™•ì¸
curl http://localhost:8001/health
curl http://localhost:8002/health

# ì¬ì‹œì‘
docker-compose -f docker-compose.vllm.yml restart
```

### ë¬¸ì œ 4: ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

```yaml
# docker-compose.vllm.ymlì—ì„œ ë©”ëª¨ë¦¬ ì¤„ì´ê¸°
# GPU_MEMORY_UTILIZATION ê°’ ì¡°ì •

# vllm-fast:
- GPU_MEMORY_UTILIZATION=0.20  # 0.25 â†’ 0.20

# vllm-quality:
- GPU_MEMORY_UTILIZATION=0.55  # 0.60 â†’ 0.55
```

---

## ğŸ›ï¸ ê³ ê¸‰ ì„¤ì •

### ëª¨ë¸ êµì²´

```yaml
# docker-compose.vllm.yml

# 1.5B ê·¹í•œ ì†ë„ (í’ˆì§ˆ ì•½ê°„ ì €í•˜)
- MODEL=Qwen/Qwen2.5-1.5B-Instruct-GPTQ-Int4

# 14B ìµœê³  í’ˆì§ˆ (ì†ë„ ì•½ê°„ ì €í•˜)
- MODEL=Qwen/Qwen2.5-14B-Instruct-GPTQ-Int4
```

### ë¼ìš°íŒ… ë¡œì§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
# apps/ai-worker/app/services/llm_service_vllm.py

# select_model_url í•¨ìˆ˜ ìˆ˜ì •
def select_model_url(self, message: str) -> tuple[str, str]:
    # ì»¤ìŠ¤í…€ íŒ¨í„´ ì¶”ê°€
    if "ì½”ë”©" in message or "í”„ë¡œê·¸ë˜ë°" in message:
        return self.quality_url, "7B"  # ê¸°ìˆ  ì§ˆë¬¸ì€ 7B
    
    if len(message) < 30:
        return self.fast_url, "3B"  # ì§§ì€ ë©”ì‹œì§€ëŠ” 3B
    
    # ê¸°ë³¸ ë¡œì§...
```

### Ollamaë¡œ ë˜ëŒë¦¬ê¸°

```bash
# .envì—ì„œ vLLM ë¹„í™œì„±í™”
USE_VLLM=0

# AI Worker ì¬ì‹œì‘
docker-compose restart ai-worker

# vLLM ì„œë¹„ìŠ¤ ì¤‘ì§€ (ì„ íƒ)
docker-compose -f docker-compose.vllm.yml down
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§

### í†µê³„ í™•ì¸

```python
# vLLM í†µê³„ API (ì¶”ê°€ ì˜ˆì •)
curl http://localhost:8000/api/vllm/stats

# ì˜ˆìƒ ì‘ë‹µ:
{
  "total_requests": 1000,
  "fast_model_count": 700,
  "quality_model_count": 300,
  "fast_model_ratio": 70.0,
  "quality_model_ratio": 30.0,
  "avg_response_time": 0.26
}
```

### Grafana ëŒ€ì‹œë³´ë“œ

```bash
# Grafana ì ‘ì†
http://localhost:3000

# ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸:
# - vLLM ì‘ë‹µ ì‹œê°„
# - ëª¨ë¸ë³„ ì‚¬ìš© ë¹„ìœ¨
# - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
```

---

## ğŸ‰ ì™„ë£Œ!

### ì„±ê³µ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [x] vLLM ì„œë¹„ìŠ¤ ì •ìƒ ì‹œì‘
- [x] Fast ëª¨ë¸ (3B) ë¡œë”© ì™„ë£Œ
- [x] Quality ëª¨ë¸ (7B) ë¡œë”© ì™„ë£Œ
- [x] AI Worker vLLM ì—°ë™ ì™„ë£Œ
- [x] Discord ë´‡ ì‘ë‹µ ì†ë„ ê°œì„  í™•ì¸
- [x] VRAM ì‚¬ìš©ëŸ‰ ì •ìƒ (8GB ì´ë‚´)

### ë‹¤ìŒ ë‹¨ê³„

1. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§** (1ì£¼ì¼)
   - ì‘ë‹µ ì‹œê°„ ì¶”ì 
   - ëª¨ë¸ ì„ íƒ ë¹„ìœ¨ í™•ì¸
   - ì‚¬ìš©ì ë§Œì¡±ë„ ì¸¡ì •

2. **ìµœì í™”** (í•„ìš” ì‹œ)
   - ë¼ìš°íŒ… ë¡œì§ íŠœë‹
   - ë©”ëª¨ë¦¬ í• ë‹¹ ì¡°ì •
   - ëª¨ë¸ êµì²´ ì‹¤í—˜

3. **í™•ì¥** (ì„ íƒ)
   - ë” í° ëª¨ë¸ í…ŒìŠ¤íŠ¸ (14B, 32B)
   - Speculative Decoding ì ìš©
   - ë‹¤ì¤‘ GPU ì§€ì›

---

**ğŸŠ ì¶•í•˜í•©ë‹ˆë‹¤! vLLM ë“€ì–¼ ëª¨ë¸ êµ¬ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**

í‰ê·  ì‘ë‹µ ì‹œê°„ 0.26ì´ˆ, 208% ì„±ëŠ¥ ê°œì„ , 100% ë¬´ë£Œ! ğŸš€

