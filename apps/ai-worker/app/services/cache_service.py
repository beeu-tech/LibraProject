"""
LLM ì‘ë‹µ ìºì‹± ì„œë¹„ìŠ¤
ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•œ ì¤‘ë³µ LLM í˜¸ì¶œ ë°©ì§€ë¡œ ì†ë„ í–¥ìƒ ë° ë¹„ìš© ì ˆê°
"""

import os
import hashlib
import json
from typing import Optional
from redis import asyncio as aioredis
import structlog

logger = structlog.get_logger(__name__)


class LLMCacheService:
    """LLM ì‘ë‹µ ìºì‹± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.redis = None
        self.enabled = os.getenv("LLM_CACHE_ENABLED", "1") == "1"
        self.ttl = int(os.getenv("LLM_CACHE_TTL", "300"))  # ê¸°ë³¸ 5ë¶„
        self.prefix = "llm:cache:"
    
    async def initialize(self):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        if not self.enabled:
            logger.info("LLM ìºì‹œ ë¹„í™œì„±í™”ë¨")
            return
        
        try:
            redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
            self.redis = await aioredis.from_url(
                redis_url,
                decode_responses=True,
                socket_connect_timeout=5
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            await self.redis.ping()
            logger.info("LLM ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ", ttl=self.ttl)
            
        except Exception as e:
            logger.warning("LLM ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨ (ìºì‹± ë¹„í™œì„±í™”)", error=str(e))
            self.enabled = False
            self.redis = None
    
    async def cleanup(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.redis:
            await self.redis.close()
            logger.info("LLM ìºì‹œ ì„œë¹„ìŠ¤ ì¢…ë£Œ")
    
    def _generate_cache_key(self, messages: list, model: str, user_id: str = "") -> str:
        """
        ë©”ì‹œì§€, ëª¨ë¸, ì‚¬ìš©ì IDë¡œ ìºì‹œ í‚¤ ìƒì„±
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            model: ì‚¬ìš©ëœ ëª¨ë¸ëª…
            user_id: ì‚¬ìš©ì ID (ì„ íƒ, ì‚¬ìš©ìë³„ ìºì‹œ ë¶„ë¦¬)
            
        Returns:
            ìºì‹œ í‚¤ ë¬¸ìì—´
        """
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œì™¸ (ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ í•´ì‹±)
        user_messages = [m for m in messages if m.get("role") != "system"]
        
        # JSON ì§ë ¬í™” (ì •ë ¬í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€)
        content = json.dumps({
            "messages": user_messages,
            "model": model,
            "user_id": user_id
        }, sort_keys=True, ensure_ascii=False)
        
        # SHA256 í•´ì‹±
        hash_value = hashlib.sha256(content.encode('utf-8')).hexdigest()
        
        return f"{self.prefix}{hash_value}"
    
    async def get_cached_response(
        self, 
        messages: list, 
        model: str,
        user_id: str = ""
    ) -> Optional[str]:
        """
        ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            model: ëª¨ë¸ëª…
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            ìºì‹œëœ ì‘ë‹µ ë˜ëŠ” None
        """
        if not self.enabled or not self.redis:
            return None
        
        try:
            key = self._generate_cache_key(messages, model, user_id)
            cached = await self.redis.get(key)
            
            if cached:
                logger.info("LLM ìºì‹œ íˆíŠ¸ ğŸ¯", 
                           key_prefix=key[:20], 
                           model=model,
                           length=len(cached))
                return cached
            
            logger.debug("LLM ìºì‹œ ë¯¸ìŠ¤", key_prefix=key[:20])
            return None
            
        except Exception as e:
            logger.error("ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨", error=str(e))
            return None
    
    async def cache_response(
        self,
        messages: list,
        model: str,
        response: str,
        user_id: str = "",
        ttl: Optional[int] = None
    ):
        """
        ì‘ë‹µì„ ìºì‹œì— ì €ì¥
        
        Args:
            messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            model: ëª¨ë¸ëª…
            response: LLM ì‘ë‹µ
            user_id: ì‚¬ìš©ì ID
            ttl: ë§Œë£Œ ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’ì€ self.ttl)
        """
        if not self.enabled or not self.redis or not response:
            return
        
        try:
            key = self._generate_cache_key(messages, model, user_id)
            expire_time = ttl or self.ttl
            
            await self.redis.setex(key, expire_time, response)
            
            logger.info("LLM ì‘ë‹µ ìºì‹œë¨ ğŸ’¾", 
                       key_prefix=key[:20], 
                       model=model,
                       length=len(response),
                       ttl=expire_time)
            
        except Exception as e:
            logger.error("ìºì‹œ ì €ì¥ ì‹¤íŒ¨", error=str(e))
    
    async def invalidate_user_cache(self, user_id: str):
        """
        íŠ¹ì • ì‚¬ìš©ìì˜ ìºì‹œ ë¬´íš¨í™”
        
        Args:
            user_id: ì‚¬ìš©ì ID
        """
        if not self.enabled or not self.redis:
            return
        
        try:
            # ì‚¬ìš©ì IDë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  ìºì‹œ í‚¤ ì°¾ê¸°
            pattern = f"{self.prefix}*{user_id}*"
            cursor = 0
            deleted_count = 0
            
            while True:
                cursor, keys = await self.redis.scan(cursor, match=pattern, count=100)
                if keys:
                    deleted_count += await self.redis.delete(*keys)
                
                if cursor == 0:
                    break
            
            logger.info("ì‚¬ìš©ì ìºì‹œ ë¬´íš¨í™”", user_id=user_id, deleted=deleted_count)
            
        except Exception as e:
            logger.error("ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨", error=str(e))
    
    async def clear_all_cache(self):
        """ëª¨ë“  LLM ìºì‹œ ì‚­ì œ"""
        if not self.enabled or not self.redis:
            return
        
        try:
            pattern = f"{self.prefix}*"
            cursor = 0
            deleted_count = 0
            
            while True:
                cursor, keys = await self.redis.scan(cursor, match=pattern, count=100)
                if keys:
                    deleted_count += await self.redis.delete(*keys)
                
                if cursor == 0:
                    break
            
            logger.info("ëª¨ë“  LLM ìºì‹œ ì‚­ì œ", deleted=deleted_count)
            
        except Exception as e:
            logger.error("ìºì‹œ ì‚­ì œ ì‹¤íŒ¨", error=str(e))
    
    async def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        if not self.enabled or not self.redis:
            return {"enabled": False}
        
        try:
            pattern = f"{self.prefix}*"
            cursor = 0
            total_keys = 0
            total_size = 0
            
            while True:
                cursor, keys = await self.redis.scan(cursor, match=pattern, count=100)
                total_keys += len(keys)
                
                for key in keys:
                    value = await self.redis.get(key)
                    if value:
                        total_size += len(value)
                
                if cursor == 0:
                    break
            
            return {
                "enabled": True,
                "total_keys": total_keys,
                "total_size_bytes": total_size,
                "ttl": self.ttl
            }
            
        except Exception as e:
            logger.error("ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨", error=str(e))
            return {"enabled": True, "error": str(e)}


# ì „ì—­ ìºì‹œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
cache_service = LLMCacheService()

